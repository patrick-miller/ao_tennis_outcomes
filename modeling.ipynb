{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured and time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.set_printoptions(threshold=50, edgeitems=20)\n",
    "\n",
    "input_dir = 'data/input/'\n",
    "\n",
    "train_ratio = 0.99\n",
    "batch_size = 128\n",
    "embedding_divisor = 1.5\n",
    "embedding_size_max = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Massage input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_feather(os.path.join(input_dir, 'train_clean'))\n",
    "data_test = pd.read_feather(os.path.join(input_dir, 'test_clean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "response_var = 'outcome'\n",
    "ignore_vars = ['id']\n",
    "cat_vars = data_train.columns[data_train.dtypes == 'category'].tolist()\n",
    "cat_vars = [var for var in cat_vars if var not in ignore_vars + [response_var]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_train = df_train.set_index(\"id\")\n",
    "# df_test = df_test.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_train, y, nas, mapper = proc_df(data_train, response_var, do_scale=True, skip_flds=ignore_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'FE': 0, 'UE': 1, 'W': 2}\n",
    "\n",
    "y_codes = [mapping.get(item) for item in y]\n",
    "\n",
    "response_y = np.eye(len(y_codes), len(mapping))[y_codes]\n",
    "# response_y = np.array([1 if item == 'W' else 0 for item in y])\n",
    "\n",
    "# response_y = np.array(y_codes)\n",
    "response_y = response_y.astype('float32')\n",
    "# response_y = response_y.astype('int64')\n",
    "# idxs = np.where(response_y > 0)[1]\n",
    "# response_y = torch.LongTensor(idxs)\n",
    "response_y = response_y.squeeze()\n",
    "response_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test, _, nas, mapper = proc_df(data_test, response_var, do_scale=True, skip_flds=ignore_vars,\n",
    "                                  mapper=mapper, na_dict=nas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Split into validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samp_size = len(df_train)\n",
    "\n",
    "val_idx = get_cv_idxs(samp_size, val_pct=1 - train_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_logloss(preds, targs, epsilon=1e-15):\n",
    "    if targs.ndim == 3:\n",
    "        targs = targs[:,0,:]\n",
    "    \n",
    "    return metrics.log_loss(targs, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ModelData object directly from the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_data = ColumnarModelData.from_data_frame(input_dir, \n",
    "   val_idx, df_train, response_y, cat_flds=cat_vars, bs=batch_size, test_df=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some categorical variables have a lot more levels than others. Store, in particular, has over a thousand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sz = [(c, len(data_train[c].cat.categories)+1) for c in cat_vars]\n",
    "cat_sz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *cardinality* of each variable (that is, its number of unique values) to decide how large to make its *embeddings*. Each level will be associated with a vector with length defined as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(c, min(embedding_size_max, int((c+1)/embedding_divisor))) for _, c in cat_sz]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_cross_entropy(input, target, weight=None, size_average=True):\n",
    "    return F.binary_cross_entropy(input, target.squeeze(1), weight=weight, size_average=size_average)\n",
    "\n",
    "def cross_entropy(input, target, weight=None, size_average=True):\n",
    "    return F.cross_entropy(input, target.squeeze(1), weight=weight, size_average=size_average)\n",
    "\n",
    "\n",
    "class StructuredClassifyLearner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        \n",
    "        self.crit = binary_cross_entropy\n",
    "        # self.crit = cross_entropy\n",
    "\n",
    "\n",
    "class MixedInputClassifyModel(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, use_bn=False):\n",
    "        super().__init__() ## inherit from nn.Module parent class\n",
    "        self.embs = nn.ModuleList([nn.Embedding(m, d) for m, d in emb_szs]) ## construct embeddings\n",
    "        for emb in self.embs: emb_init(emb) ## initialize embedding weights\n",
    "        n_emb = sum(e.embedding_dim for e in self.embs) ## get embedding dimension needed for 1st layer\n",
    "        szs = [n_emb+n_cont] + szs ## add input layer to szs\n",
    "        self.lins = nn.ModuleList([\n",
    "            nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)]) ## create linear layers input, l1 -> l1, l2 ...\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm1d(sz) for sz in szs[1:]]) ## batchnormalization for hidden layers activations\n",
    "        for o in self.lins: kaiming_normal(o.weight.data) ## init weights with kaiming normalization\n",
    "        self.outp = nn.Linear(szs[-1], out_sz) ## create linear from last hidden layer to output\n",
    "        kaiming_normal(self.outp.weight.data) ## do kaiming initialization\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(emb_drop) ## embedding dropout, will zero out weights of embeddings\n",
    "        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops]) ## fc layer dropout\n",
    "        self.bn = nn.BatchNorm1d(n_cont) # bacthnorm for continous data\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)] # takes necessary emb vectors \n",
    "        x = torch.cat(x, 1) ## concatenate along axis = 1 (columns - side by side) # this is our input from cats\n",
    "        x = self.emb_drop(x) ## apply dropout to elements of embedding tensor\n",
    "        x2 = self.bn(x_cont) ## apply batchnorm to continous variables\n",
    "        x = torch.cat([x, x2], 1) ## concatenate cats and conts for final input\n",
    "        for l, d, b in zip(self.lins, self.drops, self.bns):\n",
    "            x = F.relu(l(x)) ## dotprod + non-linearity\n",
    "            if self.use_bn: x = b(x) ## apply batchnorm activations\n",
    "            x = d(x) ## apply dropout to activations\n",
    "        x = self.outp(x) # we defined this externally just not to apply dropout to output\n",
    "\n",
    "        x = F.sigmoid(x) # for classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MixedInputClassifyModel(emb_szs = emb_szs, \n",
    "                                n_cont = len(df_train.columns) - len(cat_vars), \n",
    "                                emb_drop = 0.25, \n",
    "                                out_sz = len(mapping), \n",
    "                                szs = [1000, 500, 250], \n",
    "                                drops = [0.02, 0.04, 0.08],\n",
    "                                use_bn = True)\n",
    "\n",
    "learner = StructuredClassifyLearner(model_data, BasicModel(model),\n",
    "                                    opt_fn=optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find(1e-5, 10)\n",
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "learner.fit(lr, 3, metrics=[multi_logloss], cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# learner.save('val0')\n",
    "# learner.load('val0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds, targs = learner.predict_with_targs()\n",
    "multi_logloss(preds, targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from fastai.imports import *\n",
    "\n",
    "def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "((val, trn), (y_val, y_trn)) = split_by_idx(val_idx, df_train.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_learner = RandomForestClassifier(n_estimators=1000, max_features=0.4,\n",
    "                                    min_samples_leaf=3,\n",
    "                                    n_jobs=-1, oob_score=True, criterion='entropy')\n",
    "rf_learner.fit(trn, y_trn)\n",
    "preds = rf_learner.predict_proba(val)\n",
    "rf_learner.score(trn, y_trn), rf_learner.score(val, y_val), metrics.log_loss(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fi(rf_feat_importance(rf_learner, df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm_learner = GradientBoostingClassifier(n_estimators=2000, max_depth=3,\n",
    "                                         min_samples_leaf=3, subsample = 0.5,\n",
    "                                         learning_rate=0.01)\n",
    "gbm_learner.fit(trn, y_trn)\n",
    "preds = gbm_learner.predict_proba(val)\n",
    "gbm_learner.score(trn, y_trn), gbm_learner.score(val, y_val), metrics.log_loss(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fi(rf_feat_importance(gbm_learner, df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_winner = np.array([out if out == 'W' else 'E' for out in y])\n",
    "y_unforced = np.array([out if out == 'UE' else 'tough' for out in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((val, trn), (y_val, y_trn)) = split_by_idx(val_idx, df_train.values, y_winner)\n",
    "\n",
    "gbm_winner_learner = GradientBoostingClassifier(n_estimators=2000, max_depth=4,\n",
    "                                                min_samples_leaf=2, subsample = 0.8,\n",
    "                                                learning_rate=0.01)\n",
    "gbm_winner_learner.fit(trn, y_trn)\n",
    "preds = gbm_winner_learner.predict_proba(val)\n",
    "gbm_winner_learner.score(trn, y_trn), gbm_winner_learner.score(val, y_val), metrics.log_loss(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fi(rf_feat_importance(gbm_winner_learner, df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((val, trn), (y_val, y_trn)) = split_by_idx(val_idx, df_train.values, y_unforced)\n",
    "\n",
    "gbm_unforced_learner = GradientBoostingClassifier(n_estimators=1000, max_depth=4,\n",
    "                                                  min_samples_leaf=3, subsample = 0.6,\n",
    "                                                  learning_rate=0.01)\n",
    "gbm_unforced_learner.fit(trn, y_trn)\n",
    "preds = gbm_unforced_learner.predict_proba(val)\n",
    "gbm_unforced_learner.score(trn, y_trn), gbm_unforced_learner.score(val, y_val), metrics.log_loss(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fi(rf_feat_importance(gbm_unforced_learner, df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_trn_preds = learner.predict_dl(model_data.trn_dl)\n",
    "nn_val_preds = learner.predict_dl(model_data.val_dl)\n",
    "nn_test_preds = learner.predict_dl(model_data.test_dl)\n",
    "\n",
    "learners = {'rf': rf_learner, \n",
    "            'gbm': gbm_learner,\n",
    "            'winner': gbm_winner_learner,\n",
    "            'unforced': gbm_unforced_learner\n",
    "           }\n",
    "\n",
    "# learners = {'gbm': gbm_learner}\n",
    "\n",
    "trn_preds = [np.array(lrn.predict_proba(trn)) for nm, lrn in learners.items()] + [nn_trn_preds]\n",
    "val_preds = [np.array(lrn.predict_proba(val)) for nm, lrn in learners.items()] + [nn_val_preds]\n",
    "test_preds = [np.array(lrn.predict_proba(df_test)) for nm, lrn in learners.items()] + [nn_test_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_preds = np.column_stack(trn_preds)\n",
    "val_preds = np.column_stack(val_preds)\n",
    "test_preds = np.column_stack(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val, y_trn = split_by_idx(val_idx, y)[0]\n",
    "\n",
    "stacking_model = SGDClassifier(loss='log', penalty='l2', alpha=0.007)\n",
    "\n",
    "stacking_model.fit(trn_preds, y_trn)\n",
    "preds = stacking_model.predict_proba(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.log_loss(y_val, preds), metrics.log_loss(y_trn, stacking_model.predict_proba(trn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_nn = learner.predict(True)\n",
    "\n",
    "data_out = pd.DataFrame(pred_test_nn, columns = ['FE', 'UE', 'W'])\n",
    "data_out['submission_id'] = data_test['id'].astype('str') + '_' + data_test['gender'].astype('str')\n",
    "data_out['train'] = 0\n",
    "data_out = data_out[['submission_id', 'train', 'UE', 'FE', 'W']]\n",
    "data_out.index = data_out['submission_id']\n",
    "submission_format = pd.read_csv('data/AUS_SubmissionFormat.csv')\n",
    "data_out = data_out.loc[submission_format['submission_id']].reset_index(drop = True)\n",
    "data_out.head()\n",
    "\n",
    "data_out.to_csv('submissions/submission_nn.csv', index = False)\n",
    "data_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_stack = stacking_model.predict_proba(test_preds)\n",
    "\n",
    "data_out = pd.DataFrame(pred_test_stack, columns = ['FE', 'UE', 'W'])\n",
    "data_out['submission_id'] = data_test['id'].astype('str') + '_' + data_test['gender'].astype('str')\n",
    "data_out['train'] = 0\n",
    "data_out = data_out[['submission_id', 'train', 'UE', 'FE', 'W']]\n",
    "data_out.index = data_out['submission_id']\n",
    "submission_format = pd.read_csv('data/AUS_SubmissionFormat.csv')\n",
    "data_out = data_out.loc[submission_format['submission_id']].reset_index(drop = True)\n",
    "data_out.head()\n",
    "\n",
    "data_out.to_csv('submissions/submission_stack.csv', index = False)\n",
    "data_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_gbm = gbm_learner.predict_proba(df_test)\n",
    "\n",
    "data_out = pd.DataFrame(pred_test_gbm, columns = ['FE', 'UE', 'W'])\n",
    "data_out['submission_id'] = data_test['id'].astype('str') + '_' + data_test['gender'].astype('str')\n",
    "data_out['train'] = 0\n",
    "data_out = data_out[['submission_id', 'train', 'UE', 'FE', 'W']]\n",
    "data_out.index = data_out['submission_id']\n",
    "submission_format = pd.read_csv('data/AUS_SubmissionFormat.csv')\n",
    "data_out = data_out.loc[submission_format['submission_id']].reset_index(drop = True)\n",
    "data_out.head()\n",
    "\n",
    "data_out.to_csv('submissions/submission_gbm.csv', index = False)\n",
    "data_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test_average = (pred_test_nn + pred_test_gbm) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = pd.DataFrame(pred_test_average, columns = ['FE', 'UE', 'W'])\n",
    "data_out['submission_id'] = data_test['id'].astype('str') + '_' + data_test['gender'].astype('str')\n",
    "data_out['train'] = 0\n",
    "data_out = data_out[['submission_id', 'train', 'UE', 'FE', 'W']]\n",
    "data_out.index = data_out['submission_id']\n",
    "submission_format = pd.read_csv('data/AUS_SubmissionFormat.csv')\n",
    "data_out = data_out.loc[submission_format['submission_id']].reset_index(drop = True)\n",
    "data_out.head()\n",
    "\n",
    "data_out.to_csv('submissions/submission_average.csv', index = False)\n",
    "data_out.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "173px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
